---
title: "Fun with R:  Ecological data analysis in R"
author: "Vianney Denis"
date: "2020/11/17"
output:  
  "html_document":
   theme: united
   highlight: tango
   toc: true
   toc_depth: 4
   toc_float: true
   number_sections: false
   code_folding: show
    # ioslides_presentation 
---

# Topic - Linear Models

```{r,  eval=T, warning=F, message=F}
library(Hmisc)
library(corrplot)
library(MASS)
library(car)
```


## Simple linear regression

### `rairuoho` case study

A simple correlation among two variables

```{r,  eval=T, warning=F, message=F}
rairuoho<-read.table('Data/rairuoho.txt',header=T, sep="\t", dec=".")
cor.test(rairuoho$day6, rairuoho$day7)
```

Pearson coefficient matrix

```{r,  eval=T, warning=F, message=F}
corr<-cor(rairuoho[,1:6])
corr # cor.test does not work on Matrix
```          

Together with their significance in a better visualization

```{r,  eval=T, warning=F, message=F}
p.val<-rcorr(as.matrix(rairuoho[,1:6]))$P
corrplot(corr,type='upper',method='color', addCoef.col = "black", p.mat=as.matrix(p.val), sig.level = 0.05,title = "Correlation Matrix", mar = c(2,0,2,0), diag=F)
```

The 'relationship' (a linear model `lm`) between two variables can be added to a xy plot using the function `abline` 

```{r,  eval=T, warning=F, message=F}
plot(rairuoho$day6, rairuoho$day7)
abline(lm(rairuoho$day7~rairuoho$day6), col="red", lwd=2)
# remember `ggplot`
# ggplot(rairuoho, aes(x = day6, y = day7)) + 
#  geom_point() +
#  stat_smooth(method = "lm", col = "red")
```

Our correlation derived from a simple __linear regression__. It is used predict a quantitative outcome $y$ on the basis of one single predictor variable $x$. The goal is to build a mathematical model (or formula) that defines y as a function of the x variable.

Once, we built a statistically significant model, it’s possible to use it for predicting future outcome on the basis of new x values.

### Formula and basics

The mathematical formula of the linear regression can be written as $$ y = \beta_0 + \beta_1*x + \epsilon $$

where:

+ $\beta_0$ and $\beta_1$ are known as the regression __beta coefficients__ or __parameters__:
  + $\beta_0$ is the __intercept__ of the regression line; that is the predicted value when _x = 0_.
  + $\beta1$ is the __slope__ of the regression line.
  
+ $\epsilon$ is the __error term__ (also known as the __residual errors__)

The figure below illustrates the linear regression model, where:

+ the best-fit regression line is in blue
+ the intercept ($\beta_0$) and the slope ($\beta_1$) are shown in green
+ the error terms ($\epsilon$) are represented by vertical red lines

![](Figures/linear-regression.png)

From the scatter plot above, it can be seen that not all the data points fall exactly on the fitted regression line. Some of the points are above the blue curve and some are below it; overall, the residual errors ($\epsilon$) have approximately mean zero.

The sum of the squares of the residual errors are called the __Residual Sum of Squares__ or __RSS__.

The average variation of points around the fitted regression line is called the __Residual Standard Error (RSE)__. This is one the metrics used to evaluate the overall quality of the fitted regression model. The lower the RSE, the better it is.

Since the mean error term is zero, the outcome variable y can be approximately estimated as follow:

$$y= \beta_0+\beta_1*x$$

Mathematically, the beta coefficients ($\beta_0$ and $\beta_1$) are determined so that the RSS is as minimal as possible. This method of determining the beta coefficients is technically called __least squares__ regression or __ordinary least squares__ (OLS) regression.

Once, the beta coefficients are calculated, a t-test is performed to check whether or not these coefficients are significantly different from zero. A non-zero beta coefficients means that there is a significant relationship between the predictors ($x$) and the outcome variable ($y$).

### Computation

The simple linear regression tries to find the best line to predict $y$ on the basis of $x$.

In our `iris` data set, a linear model equation can be written as follow: $Petal.Width = \beta_0 + \beta_1 * Petal.Length$

The R function `lm` can be used to determine the beta coefficients of the linear model:

```{r,  eval=T, warning=F, message=F}
model1 <- lm(Petal.Width ~ Petal.Length, data = iris)
model1
```
The results show the intercept ($\beta_0$) and the slope ($\beta_1$), i.e. the  beta coefficients for the `Petal.Length` variable

```{r,  eval=T, warning=F, message=F}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +
  geom_point() +
  stat_smooth(method = "lm", col = "blue")
```

### Interpretation

From the output above:

+ the estimated regression line equation can be written as follow: $Petal.Width = -0.3631 + 0.4158*Petal.Length$

+ the intercept ($\beta_0$) is $-0.3631$. It can be interpreted as the predicted width of petal for a length of petal equal to zero. Regression through the origin is when you force the intercept of a regression model to equal zero. It’s also known as fitting a model without an intercept (e.g., the intercept-free linear model $y = \beta_1*x$ is equivalent to the model $y = \beta_0 + \beta_1*x$ with $\beta_0=0$). Knowing that the true relationship between your predictors and the expected value of your dependent variable has to pass through the origin would be a good reason for forcing the estimated relationship through the origin if you knew for certain what the true relationship was (very rare cases where it is justified).


+ the regression beta coefficient for the variable `Petal.Length` ($\beta1$), also known as the slope, is $0.4158$. This means that, for one unit of $Petal.Length$, we can expect an increase of $0.4158$ units in $Petal.Width$. 

### Model assessment

Before using our model to predict $Petal.Width$, you should make sure that this model is statistically significant, that is:

+ there is a statistically significant relationship between the predictor and the outcome variables

+ the model that we built fits very well the data in our hand.

### Model summary

```{r,  eval=T, warning=F, message=F}
summary(model1)
```

The summary outputs shows 6 components, including:

+ **Call** shows the function call used to compute the regression model.
+ **Residuals** provide a quick view of the distribution of the residuals, which by definition have a mean zero. Therefore, the median should not be far from zero, and the minimum and maximum should be roughly equal in absolute value.
+ **Coefficients** shows the regression beta coefficients and their statistical significance. Predictor variables, that are significantly associated to the outcome variable, are marked by stars.
+ **Residual standard error** (RSE), **R-squared** (R2) and the **F-statistic** are metrics that are used to check how well the model fits to our data.

#### Coefficients significance

The coefficients table, in the model statistical summary, shows:

+ the estimates of the **beta coefficients**
+ the **standard errors** (SE), which defines the accuracy of beta coefficients. For a given beta coefficient, the SE reflects how the coefficient varies under repeated sampling. It can be used to compute the confidence intervals and the t-statistic.
+ the **t-statistic** and the associated **p-value**, which defines the statistical significance of the beta coefficients.


**t-statistic and p-values**

For a given predictor, the t-statistic (and its associated p-value) tests whether or not there is a statistically significant relationship between a given predictor and the outcome variable, that is whether or not the beta coefficient of the predictor is significantly different from zero.

The statistical hypotheses are as follow:

+ Null hypothesis (H~0~): the coefficients are equal to zero (i.e., no relationship between x and y)
+ Alternative Hypothesis (H~1~): the coefficients are not equal to zero (i.e., there is some relationship between $x$ and $y$)

Mathematically (no need to remember), for a given beta coefficient ($\beta$), the t-test is computed as $t = (\beta - 0)/SE(\beta)$, where $SE(\beta)$ is the $\beta$ of the coefficient b. The t-statistic measures the number of standard deviations that $\beta$ is away from 0. Thus a large t-statistic will produce a small p-value.

The higher the t-statistic (and the lower the p-value), the more significant the predictor. The symbols to the right visually specifies the level of significance. The line below the table shows the definition of these symbols; one star means 0.01 < p < 0.05. The more the stars beside the variable’s p-value, the more significant the variable.

A statistically significant coefficient indicates that there is an association between the predictor ($x$) and the outcome ($y$) variable.

The t-statistic is a very useful guide for whether or not to include a predictor in a model. High t-statistics (which go with low p-values near 0) indicate that a predictor should be retained in a model, while very low t-statistics indicate a predictor could be dropped (Bruce and Bruce 2017).

**Standard errors and confidence intervals**

The standard error measures the variability/accuracy of the beta coefficients. It can be used to compute the confidence intervals of the coefficients.

For example, the 95% confidence interval for the coefficient $\beta1$ is defined as $\beta1 +/- 2*SE(\beta1)$, where:

the lower limits of $\beta_1 = \beta_1 - 2*SE(\beta_1) = 0.415 - 2*0.009 = 0.397$

the upper limits of $\beta1 = \beta1 + 2*SE(\beta1) = 0.415 - 2*0.009 = 0.435$

That is, there is approximately a 95% chance that the interval [0.397, 0.435] will contain the true value of $\beta1$. Similarly the 95% confidence interval for $\beta0 can be computed as $\beta1 +/- 2*SE(\beta1)$.

To get these information, simply type:

```{r,  eval=T, warning=F, message=F}
confint(model1)
```

#### Model accuracy


Once you identified that, at least, one predictor variable is significantly associated the outcome, you should continue the diagnostic by checking how well the model fits the data. This process is also referred to as the *goodness-of-fit*

The overall quality of the linear regression fit can be assessed using the following three parameters, displayed in the model summary:

1. **The Residual Standard Error (RSE)**

The **RSE** (also known as the model sigma) is the **residual variation**, representing the average variation of the observations points around the fitted regression line. This is the **standard deviation of residual errors**.

RSE provides an absolute measure of patterns in the **data that can’t be explained by the model**. When comparing two models, the model with the small RSE is a good indication that this model fits the best the data.

Dividing the RSE by the average value of the outcome variable will give you the prediction error rate, which should be as small as possible.

In our example, RSE = `0.2065`, meaning that the observed Petal.width values deviate from the true regression line by approximately `0.2065` units in average.

Whether or not an RSE of `0.2065` units is an acceptable prediction error is subjective and depends on the problem context. However, we can calculate the percentage error. In our data set, the mean value of Petal.Width is 1.1993, and so the percentage error is 0.2065/1.1993 = 17%.

```{r,  eval=T, warning=F, message=F}
sigma(model1)*100/mean(iris$Petal.Width)
```

2. **The R-squared ($R^2$)**

The **R-squared** $R^2$ ranges from 0 to 1 and represents the **proportion of information (i.e. variation) in the data that can be explained by the model**. The **adjusted R-squared adjusts for the degrees of freedom**.

The $R^2$ measures, how well the model fits the data. For a simple linear regression, $R^2$ is the square of the **Pearson correlation coefficient**.

A high value of R2 is a good indication. However, as the value of $R^2$ tends to increase when more predictors are added in the model, such as **in multiple linear regression model, you should mainly consider the adjusted R-squared**, which is a penalized $R^2$ for a higher number of predictors.

+ An (adjusted) $R^2$ that is close to 1 indicates that a large proportion of the variability in the outcome has been explained by the regression model.

+ A number near 0 indicates that the regression model did not explain much of the variability in the outcome.

3. **F-statistic**

The **F-statistic** gives the overall significance of the model. It assess whether **at least one predictor variable has a non-zero coefficient**.

In a simple linear regression, this test is not really interesting since it just duplicates the information in given by the t-test, available in the coefficient table. In fact, the F test is identical to the square of the t-test: $1882 = (43.387)^2$. This is true in any model with 1 degree of freedom.

The F-statistic becomes **more important** once we start **using multiple predictors** as in multiple linear regression.

A large F-statistic will corresponds to a statistically significant p-value (p < 0.05). In our example, the F-statistic equal 1882 producing a p-value of < 2.2e-16, which is highly significant.

## Multiple linear regression

**Multiple linear regression** is just an **extension of simple linear regression** used to predict an outcome variable ($y$) on the basis of multiple distinct predictor variables ($x$).

With three predictor variables ($x$), the prediction of $y$ is expressed by the following equation:

$$y = \beta_0 + \beta_1*x_1 + \beta_2*x_2 + \beta_3*x_3$$

The “$\beta$” values are called the regression weights (or beta coefficients). They measure the association between the predictor variable and the outcome. “$\beta_j$” can be interpreted as the average effect on $y$ of a one unit increase in “$x_j$”, holding all other predictors fixed.

### Building our model

We want to build a model for estimating `Petal.Width` based on data we get on `Petal.Length`, `Sepal.Length`, and `Sepal.Width`. 


$$Petal.Width = \beta_0 + \beta_1*Petal.Length + \beta_2*Sepal.Length + \beta_3*Sepal.Width$$
You can compute the model in R as follow:


```{r,  eval=T, warning=F, message=F}
model2 <- lm(Petal.Width ~ Petal.Length + Sepal.Width + Sepal.Length, data = iris)
summary(model2)
```

### Interpretation

The first step in interpreting the multiple regression analysis is to examine the F-statistic and the associated p-value, at the bottom of the model summary.

In our example, it can be seen that p-value of the F-statistic is `p-value: < 2.2e-16`, which is highly significant. This means that, at least, one of the predictor variables is significantly related to the outcome variable.

To see which predictor variables are significant, you can examine the coefficients table, which shows the estimate of regression beta coefficients and the associated t-statistic p-values:

```{r,  eval=T, warning=F, message=F}
summary(model2)$coefficient
```

For a given  predictor, the t-statistic evaluates whether or not there is significant association between this predictor and the outcome variable, that is whether the beta coefficient of the predictor is significantly different from zero.

It can be seen that, changes in `Petal.Length`,  `Sepal.Width` and `Sepal.Length`  are all significantly associated to changes in `Petal.Width`. For a given predictor variable, the coefficient ($\beta$) can be interpreted as the average effect on $y$ of a one unit increase in predictor, holding all other predictors fixed.

For example, for a fixed values of `Sepal.Width` and `Sepal.Length`, and increase of one unit of `Petal.Length` increase `Petal.Width` of `0.52408` unit on average (vs. `0.415755` unit in our example using a simple linear regression)

If a predictor was not significant in our multiple regression model, it means that its change will not significantly affect `Petal.Width`. Therefore, it is possible to remove it from the model. The selection can be done **forward** or **backward**.

As we saw earlier, the confidence interval of the model coefficient can be extracted as follow:  

```{r,  eval=T, warning=F, message=F}
confint(model2)
```

### Model accuracy assessment

As we have seen in simple linear regression, the overall quality of the model can be assessed by examining the R-squared ($R^2$) and Residual Standard Error (RSE).

1. **The R-squared ($R^2$)**

In multiple linear regression, the $R$ represents the correlation coefficient between the observed values of the outcome variable ($y$) and the fitted (i.e., predicted) values of $y$. For this reason, the value of $R$ will always be positive and will range from zero to one.

$R^2$ represents the proportion of variance, in the outcome variable y, that may be predicted by knowing the value of the $x$ variables. An $R^2$ value close to 1 indicates that the model explains a large portion of the variance in the outcome variable.

A problem with the $R^2$, is that, it will always increase when more variables are added to the model, even if those variables are only weakly associated with the response (James et al. 2014). A solution is to adjust the $R^2$ by taking into account the number of predictor variables.

The adjustment in the “Adjusted R-Squared” value in the summary output is a correction for the number of $x$ variables included in the prediction model.

2. **The Residual Standard Error (RSE).**

As mentioned earlier, the RSE estimate gives a measure of error of prediction. The lower the RSE, the more accurate the model (on the data in hand).

The error rate can be estimated by dividing the RSE by the mean outcome variable:

```{r,  eval=T, warning=F, message=F}
sigma(model2)/mean(iris$Petal.Width)
```

Which only slightly increase our prediction in comparison with using a single predictor, the 'Petal.Length` (16 vs 17 %). A parcimonial choice may be to retain  fewer  predictors if the information they bring is not important enough in our prediction.

### Model selection 

To compute multiple regression using all of the predictors in the data set, simply type this:

```{r,  eval=T, warning=F, message=F}
model3 <- lm(Petal.Width ~., data = iris[,1:4])
```

If you want to perform the regression using all of the variables except one, say `Sepal.Width`, type this:

```{r,  eval=T, warning=F, message=F}
model4 <- lm(Petal.Width ~. -Sepal.Width, data = iris[,1:4])
```

Alternatively, you can use the update function:

```{r,  eval=T, warning=F, message=F}
model5 <-  update(model2,  ~. -Sepal.Length)
```

**Information Criteria: AIC/BIC **

 An information criterion balances the fitness of a model with the number of predictors employed. Hence, it determines objectively the best model as the one that minimizes the information criterion. Two common criteria are the **Bayesian Information Criterion (BIC)** and the **Akaike Information Criterion (AIC)**. Both are based on a **balance between the model fitness and its complexity**:

$BIC(model)=-2*logLik(model) + npar(model) * log(n)$

where $Lik(model)$  is the likelihood of the model (how well the model fits the data) and $npar(model)$ is the number of parameters of the model, $k+2$ in the case of a multiple linear regression model with $k$ predictors.The AIC replaces $log(n)$
by $2$, so it penalizes less complex models. This is one of the reasons why BIC is preferred by some practitioners for model comparison. Also, because is consistent in selecting the true model: if enough data is provided, the BIC is guaranteed to select the data-generating model among a list of candidate models.

Both BIC and AIC can be computed in `R` through the functions `BIC` and `AIC`. They take a model as the input. The lower the better with a rule of thumb = 2 


```{r,  eval=T, warning=F, message=F}
BIC(model3); BIC(model4);BIC(model5)
AIC(model3); AIC(model4);AIC(model5)
```

Let’s go back to the selection of predictors. If we have $k$  predictors, a naive procedure would be to check all the possible models that can be constructed with them and then select the best one in terms of BIC/AIC. The problem is that there are $2^{k+1}$  possible models! Fortunately, the stepwise procedure helps us navigating this ocean of models. The function takes as input a model employing all the available predictors.

```{r,  eval=T, warning=F, message=F}
#Stepwise Selection based on AIC
step <- stepAIC(model3, direction='backward')
summary(step)

#Stepwise Selection with BIC
n = dim(iris[,1:4])[1]
stepBIC = stepAIC(model3,k=log(n), direction='backward')
summary(stepBIC)
```

When applying `stepAIC` for BIC/AIC, different final models might be selected depending on the choice of direction. This is the interpretation:

+ `“backward”`: starts from the full model, removes predictors sequentially.
+ `“forward”`: starts from the simplest model, adds predictors sequentially.
+ `“both”` (default): combination of the above.

The advice is to try several of these methods and retain the one with minimum BIC/AIC. Set trace = 0 to omit lengthy outputs of information of the search procedure.

### Model diagnostics and multicolinearity


Checking the assumptions of the multiple linear model through the data scatterplots becomes tricky even when $k=2$. To solve this issue, a series of diagnostic plots have been designed in order to evaluate graphically and in a simple way the validity of the assumptions. 

Our best model is: 

```{r,  eval=T, warning=F, message=F}
model3 <- lm(Petal.Width ~., data = iris[,1:4])
```

A ["diagnostic"](http://strata.uga.edu/8370/rtips/regressionPlots.html) can be printed by plotting this model 

```{r,  eval=F, warning=F, message=F}
plot(model3)
```


1. **Residuals vs. fitted values plot** This plot serves mainly to check the linearity, although lack of homoscedasticity or independence can also be detected. 

```{r,  eval=T,echo=F, warning=F, message=F}
plot(model3,1)
```

Under linearity, we expect the red line (a nonlinear fit of the mean of the residuals) to be almost flat. This means that the trend of $Y_1,...,Y_n$
is linear with respect to the predictors. Heteroscedasticity can be detected also in the form of irregular vertical dispersion around the red line. The dependence between residuals can be detected (harder) in the form of non randomly spread residuals.


2. **QQ-plot**. Check the normality 

```{r,  eval=T,echo=F, warning=F, message=F}
plot(model3,2)
```

Under normality, we expect the points (sample quantiles of the standardized residuals vs. theoretical quantiles of a  $N(0,1)$ **to align with the diagonal line**, which represents the ideal position of the points if those were sampled from a $N(0,1)$. It is usual to have larger departures from the diagonal in the extremes than in the center, even under normality, although these departures are more clear if the data is non-normal.

3. **Scale-location plot**. Serves for checking the **homoscedasticity**. It is similar to the first diagnostic plot, but now with the residuals standardized and transformed by a square root (of the absolute value). This change transforms the task of spotting heteroskedasticity by looking into irregular vertical dispersion patterns into spotting for nonlinearities, which is somehow simpler.

```{r,  eval=T,echo=F, warning=F, message=F}
plot(model3,3)
```

**Under homoscedasticity, we expect the red line to be almost flat.** If there are consistent nonlinear patterns, then there is evidence of heteroscedasticity.

4. **Standardized residuals against leverage** to detect outlier and check for the normality

```{r,  eval=T,echo=F, warning=F, message=F}
plot(model3,5)
```

On this plot, you want to see that the red smoothed line stays close to the horizontal gray dashed line and that no points have a large Cook’s distance (i.e, >0.5). 


A common problem that arises in multiple linear regression is the **multicollinearity**. This is the situation when two or more predictors are highly linearly related between them. Multicollinearitiy has important effects on the fit of the model:

+ It reduces the precision of the estimates. As a consequence, signs of fitted coefficients may be reversed and valuable predictors may appear as non significant.

+ It is difficult to determine how each of the highly related predictors affects the response, since one masks the other. This may result in numerical instabilities.

An approach is to detect multicollinearity is to compute a correlation matrix between the predictors as we learned earlier

```{r,  eval=T, echo=F, warning=F, message=F}
corr<-cor(iris[,1:3])
p.val<-rcorr(as.matrix(rairuoho[,1:3]))$P
corrplot(corr,type='upper',method='color', addCoef.col = "black", p.mat=as.matrix(p.val), sig.level = 0.05,diag=F)
```

Here we can see what we already knew that `Sepal.Length` and `Petal.Length` are highly linearly related. One could be removed. However, it is not enough to inspect pair by pair correlations in order to get rid of multicollinearity.A better approach is to compute the **Variance Inflation Factor** (VIF) of each coefficient $\beta_j$. This is measure of how linearly dependent is $X_j$ with the rest of predictors:


$$\text{VIF}(\beta_j)=\frac{1}{1-R^2_{X_j|X_{-j}}}$$

where $R^2_{X_j|X_{-j}}$ is the $R^2$ from a regression of $X_j$ into the remaining predictors. The next rule of thumb gives direct insight into which predictors are multicollinear:

 + **VIF close to 1**: absence of multicollinearity.
 + **VIF larger than 5 or 10**: multicolinearity **problematic**.  Advised to remove the predictor with largest VIF.


VIF is called by function `vif` and takes as an argument a linear model

```{r,  eval=T, warning=F, message=F}
vif(model3)
#remove Petal.Length
model6 <- lm(Petal.Width ~. -Petal.Length, data = iris[,1:4])
vif(model6)
plot(model6)
```      

That's our best model for today :)

**References:**

Bruce, Peter, and Andrew Bruce (2017). Practical Statistics for Data Scientists. [O’Reilly Media](https://www.amazon.com/Practical-Statistics-Data-Scientists-Essential/dp/149207294X).

James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated.





